# alignment/embedding_aligner_base.py
import torch
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class EmbeddingAligner(ABC):
    """
    Abstract base class for aligning an input embedding with a set of text descriptions.
    Assumes the input embedding is already in a space semantically comparable 
    and has the SAME DIMENSION as the text embeddings generated by the concrete subclass.
    """
    def __init__(self, descriptions: Dict[str, str], device: Optional[torch.device] = None) -> None:
        """
        Initializes the aligner.

        :param descriptions: Dictionary mapping unique names to text descriptions.
        :param device: The torch device to use (e.g., 'cuda', 'cpu'). Auto-detects if None.
        """
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        # Logger info moved to concrete class init for more context
        
        if not descriptions:
            raise ValueError("Descriptions dictionary cannot be empty.")
            
        self.descriptions = descriptions
        self.action_names: List[str] = list(descriptions.keys())
        
        # Pre-compute text embeddings and store embedding dimension
        # This calls the concrete _encode_texts method
        self.text_embeddings: torch.Tensor = self._precompute_text_embeddings()
        
        # Store the embedding dimension from the precomputed text embeddings
        self.embed_dim: int = self.text_embeddings.shape[1] 
        logger.info(f"Aligner initialized on device '{self.device}'. Pre-computed {len(self.action_names)} text embeddings with dimension {self.embed_dim}.")

    @abstractmethod
    def _encode_texts(self, texts: List[str]) -> torch.Tensor:
        """
        Encodes a list of text strings into an embedding tensor. 
        Must be implemented by subclasses.

        :param texts: A list of text strings to encode.
        :return: A tensor of shape [num_texts, embedding_dim]. Normalization is recommended.
        """
        pass

    def _precompute_text_embeddings(self) -> torch.Tensor:
        """
        Computes and stores embeddings for all descriptions using the subclass's encoder.
        Ensures final embeddings are normalized and on the correct device.
        
        :return: A tensor of normalized text embeddings [num_descriptions, embedding_dim].
        """
        all_texts = [self.descriptions[name] for name in self.action_names]
        if not all_texts:
            raise ValueError("No text descriptions found to precompute embeddings.")
             
        with torch.no_grad():
            # Call the concrete implementation (e.g., CLIP or SBERT encoder)
            embeddings = self._encode_texts(all_texts) 
        
        if not isinstance(embeddings, torch.Tensor) or embeddings.ndim != 2:
            raise ValueError(f"Subclass _encode_texts must return a 2D tensor, got {type(embeddings)} with shape {embeddings.shape if isinstance(embeddings, torch.Tensor) else 'N/A'}")
        if embeddings.shape[0] != len(self.action_names):
            raise ValueError(f"Number of computed embeddings ({embeddings.shape[0]}) does not match number of descriptions ({len(self.action_names)}).")

        embeddings = embeddings.to(self.device)
        # Normalize embeddings for cosine similarity calculation
        embeddings_norm = embeddings / (embeddings.norm(dim=-1, keepdim=True) + 1e-9) # Add epsilon for stability
        return embeddings_norm

    def _calculate_similarity(self, input_embedding_normalized: torch.Tensor) -> torch.Tensor:
        """
        Calculates cosine similarity between a normalized input embedding and text embeddings.

        :param input_embedding_normalized: A *normalized* input embedding tensor [1, self.embed_dim].
        :return: A tensor of similarity scores [num_descriptions].
        """
        # self.text_embeddings are already normalized during precomputation
        # Input should already be checked for dimension compatibility before normalization
        similarity = torch.mm(input_embedding_normalized, self.text_embeddings.T) # [1, D] @ [D, N] -> [1, N]
        return similarity.squeeze(0) # Return shape [N]

    def _validate_and_normalize_input(self, input_embedding: torch.Tensor) -> torch.Tensor:
        """
        Validates shape, checks dimension, moves to device, and normalizes the input embedding.

        :param input_embedding: The raw input embedding tensor, shape [1, self.embed_dim].
        :return: The validated, normalized embedding on the correct device, shape [1, self.embed_dim].
        :raises ValueError: If shape or dimension is incorrect.
        """
        if not isinstance(input_embedding, torch.Tensor):
            raise TypeError(f"Input embedding must be a torch.Tensor, got {type(input_embedding)}")
             
        if not (input_embedding.ndim == 2 and input_embedding.shape[0] == 1):
            raise ValueError(f"Input embedding must have shape [1, embed_dim], got {input_embedding.shape}")
        
        # Check dimension against the pre-computed text embedding dimension
        if input_embedding.shape[1] != self.embed_dim:
            raise ValueError(
                f"Input embedding dimension ({input_embedding.shape[1]}) does not match "
                f"the text embedding dimension ({self.embed_dim}) required by this aligner. "
                f"The input embedding (e.g., from PointNet) must be projected to {self.embed_dim} dimensions FIRST."
            )
            
        input_embedding = input_embedding.to(self.device)
        input_embedding_norm = input_embedding / (input_embedding.norm(dim=-1, keepdim=True) + 1e-9)
        return input_embedding_norm

    def align(self, input_embedding: torch.Tensor) -> Tuple[str, float]:
        """
        Finds the best matching description/action name for the given input embedding.
        The input embedding MUST have the same dimension as the aligner's text embeddings 
        and be in a semantically comparable space (e.g., PointNet embedding projected to CLIP space).

        :param input_embedding: The input embedding tensor, shape [1, self.embed_dim].
        :return: A tuple containing the best matching action name (str) and its similarity score (float).
        :raises ValueError: If input embedding shape or dimension is incorrect.
        """
        input_embedding_norm = self._validate_and_normalize_input(input_embedding)
            
        with torch.no_grad():
            # Compare normalized input against normalized precomputed text embeddings
            similarities = self._calculate_similarity(input_embedding_norm) 
        
        best_match_score, best_match_idx = similarities.max(dim=0)
        best_action_name = self.action_names[best_match_idx.item()]
        
        return best_action_name, best_match_score.item()

    def get_similarities(self, input_embedding: torch.Tensor) -> Dict[str, float]:
        """
        Calculates similarity scores for all descriptions relative to the input embedding.
        The input embedding MUST have the same dimension as the aligner's text embeddings
        and be in a semantically comparable space.

        :param input_embedding: The input embedding tensor, shape [1, self.embed_dim].
        :return: A dictionary mapping action names to similarity scores.
        :raises ValueError: If input embedding shape or dimension is incorrect.
        """
        input_embedding_norm = self._validate_and_normalize_input(input_embedding)
             
        with torch.no_grad():
            similarities = self._calculate_similarity(input_embedding_norm) 
            
        return {name: sim.item() for name, sim in zip(self.action_names, similarities)}