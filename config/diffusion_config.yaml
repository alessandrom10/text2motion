run_name: "diffusion_experiment_01"

paths:
  data_root_dir: "C:/PythonProjects/text2motion/data/"
  armature_config_file: "config/armature_config.json" # Relative to project root
  annotations_file_name: "annotations_train.csv" # Relative to data_root_dir
  val_annotations_file_name: "annotations_val.csv" # Relative to data_root_dir
  motion_subdir: "new_joints" # Subdirectory within data_root_dir
  model_save_dir: "C:/PythonProjects/text2motion/trained_models_final" # Absolute path for saving models
  model_filename: "armature_mdm_final.pth"
  precomputed_sbert_train_path: "./data/sbert_embeddings_train.pt" # Relative to project root or ensure full path
  precomputed_sbert_val_path: "./data/sbert_embeddings_val.pt"   # Relative to project root or ensure full path

diffusion_hyperparameters:
  num_diffusion_timesteps: 1000 # SUGGESTION: Increased from 100. Monitor training time.
                               # MDM paper uses 1000.

model_hyperparameters:
  num_motion_features: 66
  latent_dim: 768
  ff_size: 1024
  num_layers: 8
  num_heads: 4
  dropout: 0.1
  sbert_model_name: 'all-mpnet-base-v2'
  sbert_embedding_dim: 768
  max_armature_classes: 10
  armature_embedding_dim: 64
  text_time_cond_policy: "add"
  armature_integration_policy: "add" # Current setting. Test "add" if "concat" seems problematic.
  timestep_embedder_type: "mlp"
  text_cond_dropout_prob: 0.1
  armature_cond_dropout_prob: 0.1 # SUGGESTION: Was 0.2. If issues with armature conditioning, try 0.1 or 0.15.
  max_seq_len_pos_enc: 5000
  use_final_algebraic_refinement_encoder: true
  algebraic_refinement_hidden_dim: null
  timestep_gru_num_layers: 1
  timestep_gru_dropout: 0.0
  num_joints_for_geom: 22
  features_per_joint_for_geom: 3

training_hyperparameters:
  batch_size: 32 # SUGGESTION: Was 128. Reduced for potentially more stable gradients & less memory. Adjust based on GPU.
  num_epochs: 50 # Or more, if static output persists and loss is still decreasing.
  learning_rate: 0.0005 # SUGGESTION: Slightly reduced from 0.001, especially if enabling LR scheduler or if loss is unstable.
  cfg_drop_prob_trainer: 0.15 # SUGGESTION: Was 0.2. Slight reduction to balance conditional/unconditional.
  device: "cuda"
  use_lr_scheduler: true # SUGGESTION: Enabled.
  lr_scheduler_factor: 0.5
  main_loss_type_trainer: "mse"


  generate_sample_every_n_epochs: 2  # Generate a sample every 5 epochs. Set to 0 to disable.
  sample_generation_prompt: "a person is jumping excitedly" # Text prompt for the sample
  sample_generation_armature_id: 1    # Armature ID for the sample
  sample_generation_num_frames: 1000   # Number of frames for the sample animation
  sample_generation_cfg_scale: 2.5

main_x0_loss_config:
  timestep_weighting:
    scheme: "snr_plus_one" # Keep this for now, as it's a good scheme to try.
    min_snr_gamma_value: 5.0

early_stopping:
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0001
      
kinematic_losses:
  use_kinematic_losses: true # Keep false initially. Enable if motion lacks smoothness after other changes.
  velocity_loss_weight: 0.05   # Reduced example weight if activated
  acceleration_loss_weight: 0.05 # Reduced example weight if activated
  kinematic_loss_type: "l1"

mdm_geometric_losses:
  use_mdm_geometric_losses: true
  lambda_pos: 0.05  # SUGGESTION: Slightly reduced from 0.1, to give velocity more relative importance.
  lambda_vel: 0.1  # SUGGESTION: Drastically reduced from 0.5. If this is too low and motion is wild, increase.
                     # The goal is to see if reducing its penalty allows dynamics to emerge.
  lambda_foot: 0.0  # Keep at 0.0 if foot_contact_gt is not available or reliable.
                     # If available, try 0.5 or 1.0.
  foot_joint_indices: [10, 11]

dataset_parameters:
  min_seq_len_dataset: 20 # SUGGESTION: Increased from 5. Very short sequences might not capture enough dynamics.
  max_seq_len_dataset: 120 # SUGGESTION: Increased from 100 slightly, if memory allows, for more context.
  dataset_device: 'cpu'